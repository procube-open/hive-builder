---
- name: check kernel version
  shell: uname -r
  register: uname_result
  check_mode: False
  changed_when: False
# Ansible module unarchive supports extracting files from URL.
# But there is not a way how to increase timeout in case of network
# issues. Module get_url has default timeout 10 seconds for URL requests
# and it can be changed.
# https://github.com/CentOS-PaaS-SIG/contra-env-setup/pull/12
- name: download drbd rpm tarball
  get_url:
    url: >-
      https://github.com/procube-open/drbd9-rpm/releases/download/{{
        ('9.0.22/drbd9-rpm-amzn2' if uname_result.stdout is version('4.14.203', 'lt')
         else '9.0.25/drbd9-rpm-amzn2' if uname_result.stdout is version('4.14.219', 'lt')
         else '9.0.27/drbd9-rpm-amzn2'  if uname_result.stdout is version('4.14.225', 'lt')
         else '9.0.28/drbd9-rpm-amzn2_225_169.362'
        ) if hive_provider == 'aws'
        else '9.0.20/drbd9-rpm' if uname_result.stdout is version('3.10.0-1127', 'lt')
        else '9.0.24/drbd9-rpm' if uname_result.stdout is version('4.18.0', 'lt')
        else '9.0.25/drbd9-rpm-foros8'}}.tar.gz
    dest: /root/drbd9-rpm.tar.gz
- name: extract drbd rpm files
  unarchive:
    src: /root/drbd9-rpm.tar.gz
    remote_src: yes
    dest: /root/
- name: gather rpm file list
  shell: ls -1 /root/RPMS{{ '.amzn2' if hive_provider == 'aws' else '' if uname_result.stdout is version('4.18.0', 'lt') else '.foros8' }}/*/*.rpm
  check_mode: False
  changed_when: False
  register: hive_safe_rpms
- name: install packages
  yum:
    state: present
    name: "{{ hive_safe_rpms.stdout_lines }}"
    disable_gpg_check: true
  register: drbd_install
- name: install LVM tools
  yum:
    state: present
    name: lvm2
- name: change usage count setting
  lineinfile:
    regexp: "usage-count .*;"
    line: "        usage-count no;"
    path: /etc/drbd.d/global_common.conf
    state: present
- name: change usage count setting
  lineinfile:
    insertafter: "# minor-count"
    line: "        minor-count 128;"
    path: /etc/drbd.d/global_common.conf
    state: present

- name: check volume group exists
  command: vgdisplay drbdvg
  failed_when: False
  register: vgdisplay_result
  check_mode: False
  changed_when: False

- name: check devices
  parted:
    device: "{{ item }}"
    unit: s
  register: hive_safe_devices
  loop:
    - /dev/sdc
    - /dev/sdb
    - /dev/vdc
    - /dev/vdb
    - /dev/xvdb
    - /dev/nvme1n1
    - /dev/sda
  when: not (hive_no_mirrored_device | default(False)) and vgdisplay_result.rc != 0
  failed_when: False
- set_fact:
    hive_safe_mirrored_device: "{{ hive_safe_devices.results | selectattr('partitions','defined') |first }}"
  when: not (hive_no_mirrored_device | default(False)) and vgdisplay_result.rc != 0
# from https://askubuntu.com/questions/201164/proper-alignment-of-partitions-on-an-advanced-format-hdd-using-parted
# Parted is just being overly conservative. The usual practice these days is to align partitions
# on 1MiB (2048-sector) boundaries because this works on Advanced Format disks, on certain type
# of RAID setups that require alignment, and on most SSDs. For an Advanced Format disk, so long
# as the alignment is on a multiple of 8, you're fine, and 2048 is a multiple of 8. The lost
# disk space is puny -- 0.0000336% of your total disk space, if I did the math right and didn't
# mistype anything. So don't worry about it; just use the 1MiB alignment.
- name: create partition for drbd resource pool
  parted:
    label: "{{ hive_partition_label | default(omit)}}"
    device: "{{ hive_safe_mirrored_device.disk.dev }}"
    number: "{{ hive_safe_mirrored_device.partitions | length + 1 }}"
    part_end: "{{ ((((hive_safe_mirrored_device.disk.size | int) - 34) // 2048) | string) + 'MiB' }}"
    part_start: "{{ '1MiB' if (hive_safe_mirrored_device.partitions | length) == 0 else (((((hive_safe_mirrored_device.partitions | last).end | int) // 2048 ) + 1) | string) + 'MiB' }}"
    flags: [ lvm ]
    state: present
    unit: MiB
  register: hive_safe_parted
  when: not (hive_no_mirrored_device | default(False)) and vgdisplay_result.rc != 0
- name: create volume group
  lvg:
    vg: drbdvg
    pvs: "{{ hive_safe_mirrored_device.disk.dev }}{% if hive_safe_mirrored_device.disk.dev is match('/dev/nvme*') %}p{% endif %}{{ hive_safe_mirrored_device.partitions | length + 1 }}"
  when: not (hive_no_mirrored_device | default(False)) and vgdisplay_result.rc != 0
- name: create a thin pool
  lvol:
    vg: drbdvg
    opts: "--chunksize 1024K --zero n"
    thinpool: drbdpool
    # lvcreate calculate metadata size as ((Pool_LV_size / Pool_LV_chunk_size * 64) // 4096M + 1) * 4096M
    # lvcreate calculate pool metadata spare size as same as metadata size
    # When Pool_LV_chunk_size = 1024K and Pool_LV_extent_size=16M, Pool_LV_size = (VG_size * 8192 // 8193) // 16M * 16M
    # if VG_size is not enough then cause error: lvcreate: metadata/pv_map.c:198: consume_pv_area: Assertion `to_go <= pva->count' failed.
    size: "{{ (((hive_safe_parted.partitions | last).size | int) - 8) * 8192 // 8193 // 16 * 16 }}M"
  when: not (hive_no_mirrored_device | default(False)) and vgdisplay_result.rc != 0
- import_tasks: iptables.yml
  vars:
    hive_safe_iptables_name: drbd
    hive_safe_iptables_dports:
      - 7000:7999
    hive_safe_iptables_protocol: tcp

- name: "install drbd resource service template"
  copy:
    src: drbd-resource@.service
    dest: /usr/lib/systemd/system/
- name: "install waiting drbd device tool"
  copy:
    src: waitdevice
    dest: /usr/bin/
    mode: 0755
